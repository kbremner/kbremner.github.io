---
layout: post
title: Scraping Kickstarter - Part 1 - Kyle Bremner | Software Engineer
item_title: Scraping Kickstarter - Part 1
sections: 
  - Getting Projects
  - Searching
  - More Information
  - Summary
tags: android java kickstarter scraping api
published: true
---

Having backed a few projects on Kickstarter, it was a bit disappointing when they released a new iPhone application, but nothing for Android users. There are a couple of third party apps on Google Play, however I thought I'd take a shot at scraping Kickstarter and seeing how much information I could get, and what I could create. This post is the first in a series following my progress.


{% include section_header.html name="Getting Projects" %}
###Finding the Data
To get the information we need, we could grab the web pages and scrape the DOM. However, this requires downloading a lot of markup that we don't need, as well as presenting difficulties in how to deal with content generated by JavaScript. It is much easier and more efficient to try and obtain the data from a data endpoint. This isn't possible in all cases, but it's always worth trying to find one by poking around the site to be scraped.

So lets have a look at the Kickstarter site. A list of all projects on Kickstarter can be viewed by going to [https://www.kickstarter.com/discover/advanced](https://www.kickstarter.com/discover/advanced). Changing one of the filters at the top of the page results in it being updated dynamically, meaning that the page must be grabbing the data from somewhere. When investigating the network traffic, it can be seen that changing a filter causes a GET request to be sent to the same URL, but with some URL parameters added and the `Accept` header set to `application/json`. The addition of the header results in JSON data being returned instead of a web page. In fact, the `Accept` header isn't needed - the same effect can be achieved by adding .json to the URL, i.e. [https://www.kickstarter.com/discover/advanced.json](https://www.kickstarter.com/discover/advanced.json).

It also turns out that the results are paginated. Scrolling to the bottom of the page and pressing "load more" will result in a request being sent with a `page` URL parameter. The parameter value can be incremented until the returned JSON data contains no projects, meaning that all the results have been returned. But what other parameters can we use?

###Endpoint Parameters
As we've seen above, each filter is associated with a URL parameter that is set when sending the request. Changing the category to Art, for instance, results in the `category_id` URL parameter being added, set to 1. Also, changing the sort type to popularity results in the `sort` URL parameter being added, set to popularity. There is a plus button next to the main filters, which can be used to add additional filters. From this, we can build up a list of all the possible parameters and the values that they can be set to.

However, some parameters are more difficult to work with than others... If we press on the location filter, it opens a pop-over element. After choosing a location, we can see that a `woe_id` URL parameter is added, set to an ID representing that location. But how did the page get that ID?


{% include section_header.html name="Searching" %}
When we enter text in the location search box, we can see that a request is sent to a new endpoint, with a URL parameter specifying the search term(s), i.e. [https://www.kickstarter.com/locations/search.json?searchable=true&term=london+UK](https://www.kickstarter.com/locations/search.json?searchable=true&term=london+UK). The returned JSON data contains locations that match the search terms. The ID in one of the locations can then be used as the value for the `woe_id` URL parameter.

So we know we can search locations, but it also turns out that there's a similar endpoint for searching for projects. At the top of the page is a search box for finding projects. Entering text in it results in a request being sent to yet another endpoint, in a similar format to the location search, i.e. [https://www.kickstarter.com/projects/search.json?term=space+monkey](https://www.kickstarter.com/projects/search.json?term=space+monkey). The returned JSON data contains details about projects that match the search terms, giving us a means of easily searching for projects.


{% include section_header.html name="More Information" %}
For each project, we get some interesting details - how many backers it has, how much it's raised, and some information about it's creator and location. But what more can we get?

In the details for each project is a URL to a profile page for it's creator. Trying our previous trick on this URL doesn't work though - trying to get JSON data just results in a 404 error page being returned. Viewing the network traffic for the page doesn't reveal anything interesting either. There is still the option of scraping the profile page, but at the moment, we'll have to make do with the information contained in the project details about the creator.

There are also two URLs associated with the project's location. This time, our trick works on these URLs. The first one provides details about other projects that share the same location (a quick test shows that you can use the URL parameters from above on this URL as well). The other gives details of locations that are near the project's location.

So what about the project itself? Well, there is another URL associated with it. Again, our trick does work. However, the JSON data contains chunks of HTML. There is useful information to be gained from these chunks, but we'll leave that investigation for a future post.

{% include section_header.html name="Summary" %}
Now we have a means of getting basic details about projects, as well as getting more specific information about individual projects. in the next post, we can start having a look at actually using the information.
